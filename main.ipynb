{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import string\n",
    "import re\n",
    "import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import time\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from gensim.models import Word2Vec, FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will load the data\n",
    "data = json.load(open('data.json'))\n",
    "\n",
    "# now we extract sentences with events in them \n",
    "# (we are implicitly assuming here that we can exctract events without looking at sentences before or after the one \n",
    "# with the event in it)\n",
    "text = [] \n",
    "lines = []\n",
    "anchor_types = [] \n",
    "anchor_text = [] \n",
    "\n",
    "for document in data:\n",
    "    for event in document['events']:\n",
    "        for mention in event['MENTIONS']:\n",
    "            lines.append(mention['scope_text'].replace('\\n', ' '))\n",
    "            anchor_types.append(event['TYPE'])\n",
    "            anchor_text.append(mention['anchor_text'].replace('\\n', ' '))\n",
    "            \n",
    "    text.append(re.sub(r'<.{1,10}>', '', document['text'].replace('\\n', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Even as the secretary of homeland security was putting his people on high alert last month, a 30-foot Cuban patrol boat with four heavily armed men landed on American shores, utterly undetected by the Coast Guard Secretary Ridge now leads', \"And these bozos let four armed Cubans land on our shores when they're trying to make a high terrorist alert\", 'He lost an election to a dead man']\n",
      "\n",
      "['landed', 'land', 'election']\n",
      "\n",
      "['Movement', 'Movement', 'Personnel']\n"
     ]
    }
   ],
   "source": [
    "print(lines[:3])\n",
    "print()\n",
    "print(anchor_text[:3])\n",
    "print()\n",
    "print(anchor_types[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A lot of sentences contain more than one event (right now they are stored as a separate entities in \"lines\", \"anchor_type\" and \"anchor_text\"). Let's regroup our dataset accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if(data.get(lines[i]) is not None):\n",
    "        data[lines[i]]['anchor_type'].append(anchor_types[i])\n",
    "        data[lines[i]]['anchor_text'].append(anchor_text[i])\n",
    "    else:\n",
    "        data[lines[i]] = {'anchor_type':[anchor_types[i]], 'anchor_text':[anchor_text[i]]}\n",
    "    \n",
    "pre_dataset = []\n",
    "for key, value in data.items():\n",
    "    pre_dataset.append({'text':key, 'anchors_type':value['anchor_type'], 'anchors_text':value['anchor_text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4382\n",
      "\n",
      "{'text': 'Even as the secretary of homeland security was putting his people on high alert last month, a 30-foot Cuban patrol boat with four heavily armed men landed on American shores, utterly undetected by the Coast Guard Secretary Ridge now leads', 'anchors_type': ['Movement'], 'anchors_text': ['landed']}\n",
      "\n",
      "{'text': \"We're talking about possibilities of full scale war with former Congressman Tom Andrews, Democrat of Maine\", 'anchors_type': ['Conflict', 'Personnel'], 'anchors_text': ['war', 'former']}\n",
      "\n",
      "{'text': 'Let me tell you, what trips to Walter Reed taught me was, that whoever thought up the term, the law of unintended consequences it pertains to war', 'anchors_type': ['Conflict', 'Movement'], 'anchors_text': ['war', 'trips']}\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size: {}\\n'.format(len(pre_dataset)))\n",
    "print(pre_dataset[0])\n",
    "print()\n",
    "print(pre_dataset[11])\n",
    "print()\n",
    "print(pre_dataset[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready to prepocess text into something pytorch will understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# two tokens are special. One is used if we will find token in our dataset that we do not have\n",
    "# in dictionary of pretrained GloVe vectros. Second one is used so that we can process \n",
    "# sentences in batches (we padd them so that lenght is the same)\n",
    "word2vec = [np.zeros(50), np.random.randn(50)]\n",
    "word2id = { '__PADDING__':0, '__UNKNOWN__':1}\n",
    "id2word = ['__PADDING__', '__UNKNOWN__']\n",
    "i = 2\n",
    "\n",
    "anchor_type_to_id = {name:i for name, i in zip(np.unique(anchor_types), range(len(np.unique(anchor_types))))}\n",
    "id_to_anchor_type = np.unique(anchor_types)\n",
    "\n",
    "with open('/media/sf_win/embeddings/glove.6B.50d.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        splitted = line.split()\n",
    "        token = splitted[0]\n",
    "        \n",
    "        word2vec.append(np.array(splitted[1:]).astype(float))\n",
    "        word2id[token] = i\n",
    "        id2word.append(token)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for sample in pre_dataset:\n",
    "    parsed = parser(sample['text'])\n",
    "    \n",
    "    Input = np.array([word2id[token.text] if (token.text in word2id) else word2id['__UNKNOWN__'] for token in parsed])\n",
    "    anchors = np.array([anchor_type_to_id[anchor] for anchor in sample['anchors_type']])\n",
    "    \n",
    "    tokenized_line = np.array([token.text for token in parsed])\n",
    "    \n",
    "    anchors_position = np.array([np.argmax(tokenized_line == anchor) for anchor in sample['anchors_text']])\n",
    "    \n",
    "    dataset.append({'input':Input, 'anchors_types':anchors, 'anchors_position':anchors_position})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will add to each sentence a padding token to the beginning. This is necessary because \n",
    "# we will search for events sequentialy - one by one. And if our model points at the first token\n",
    "# in sentence (the padding token), we will assume there are no more events to find\n",
    "# Just for convinience we will introduce special type for this \"ending event\" = -1\n",
    "for sample in dataset:\n",
    "    sample['input'] = [np.hstack([[word2id['__PADDING__']], sample['input']])]  # additional [] because torch \n",
    "    sample['anchors_position'] = sample['anchors_position'] + 1                 # only works with bactes, even\n",
    "    sample['anchors_position'] = [np.hstack([sample['anchors_position'], [0]])] # if batch_size = 1\n",
    "    sample['anchors_types'] = [np.hstack([sample['anchors_types'], [-1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': [array([    0,     1,    21,     2,   499,     5,  3946,   196,    17,\n",
       "          2222,    28,    71,    15,   154,  3641,    78,   231,     3,\n",
       "             9, 36064,     1,  3475,  2379,    19,   135,  2269,  1104,\n",
       "           303,  3804,    15,     1, 10264,     3, 14307, 28657,    23,\n",
       "             2,     1,     1,     1,     1,   116,  2392])],\n",
       " 'anchors_types': [array([ 5, -1])],\n",
       " 'anchors_position': [array([28,  0])]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_dim = 100, dropout=0.3, num_layers=2, batch_size=1):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # here we will store a log probabilities of chosen actions\n",
    "        self.saved_log_prob = []\n",
    "        \n",
    "        # len(word2vec[0]) = embedding_dim\n",
    "        # this transforms [batch, sentence_length] to [batch, sentence_length, embedding_dim]\n",
    "        self.embeddings = nn.Embedding(len(word2vec), len(word2vec[0]), _weight=torch.FloatTensor(word2vec))\n",
    "        \n",
    "        # we don't wont to train already trained embeddings. Besides, this is so much faster :)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        \n",
    "        # this lstm will learn to point anchor position. \n",
    "        # It transforms [batch, sentence_length, embedding_dim] to [batch, sentence_length, hidden_dim]\n",
    "        self.lstm_pointer = nn.LSTM(len(word2vec[0]), \n",
    "                                    hidden_dim // 2, # because bidirectional\n",
    "                                    dropout=dropout, \n",
    "                                    bidirectional=True,\n",
    "                                    num_layers=num_layers,\n",
    "                                    batch_first=True) # batch_size first so that we don't have to permute our tensors\n",
    "        self.hidden_for_pointer = self.init_hidden()\n",
    "        \n",
    "        # tipicaly here we would add a linear layer on top of lstm with transformation\n",
    "        # [batch, sentence_length, hidden_dim] -> [batch, sentence_length, 1]\n",
    "        # But we would like our prediction to be influenced by previously predicted \n",
    "        # anchors. So insted we will pass all previous predictions into forward function, multiply\n",
    "        # our hidden tensor by it (our previous predictions will be a tensor of shape \n",
    "        # [number_of_prediction, hidden_dim]) and then sum over [number_of_prediction] dimension\n",
    "        # Notice that this scheme only works if we add 'zero_prediction' (so we can make first prediction). \n",
    "        # It will be a constant vector of ones (normalized) \n",
    "        \n",
    "        # and this one will learn to guess types of each token as if it is an anchor\n",
    "        # It transforms [batch_size, sentence_length, embedding_dim] to [batch_size, sentence_length, hidden_dim]\n",
    "        self.lstm_typer = nn.LSTM(len(word2vec[0]), \n",
    "                                  hidden_dim // 2, # because bidirectional\n",
    "                                  dropout=dropout, \n",
    "                                  bidirectional=True,\n",
    "                                  num_layers=num_layers,\n",
    "                                  batch_first=True)\n",
    "        self.hidden_for_typer = self.init_hidden()\n",
    "        \n",
    "        # this will transform [batch, sentence_length, hidden_dim] to [batch, sentence_length, 7]\n",
    "        self.linear_over_typer = nn.Linear(hidden_dim, 7)\n",
    "    \n",
    "        #the following is done for computational efficiency:\n",
    "        self.encoded_integers_array = np.array(list(map((lambda n: self.encode_integer(n, self.hidden_dim // 2)), \n",
    "                                                        np.arange(0, 100))))\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim // 2)),\n",
    "                Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim // 2)))    \n",
    "        \n",
    "    def reinit_hidden(self):\n",
    "        self.hidden_for_pointer = self.init_hidden()\n",
    "        self.hidden_for_typer = self.init_hidden()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "            This returns tensor of [batch_size, sentence_lenght, number_of_anchor_types=7],\n",
    "            where each element in batch the probability of token in *position* \n",
    "            be an *anchor_type* is [*batch*, *position*, *anchor_type*]. \n",
    "            probabilities normalized for each element in batch (so that sum(output) = batch_size)\n",
    "            \n",
    "            'state' is a tuple, where state[0] is an input sentence (numpy array of type int and shape \n",
    "            [batch_size, sentence_length]) and state[1] is a list of actions \n",
    "            (Each action is a pair of integer numbers (anchor_position, anchor_type))\n",
    "        \"\"\"\n",
    "        \n",
    "        Input = torch.LongTensor(state[0]).detach()\n",
    "        \n",
    "        embeded = self.embeddings(Input)\n",
    "        \n",
    "        ###############  first we predict a best position for anchor        ################\n",
    "        lstm_pos_out, self.hidden_for_pointer = self.lstm_pointer(embeded, self.hidden_for_pointer)\n",
    "        \n",
    "        actions = torch.FloatTensor(self.encode_actions(state[1])).detach()\n",
    "        \n",
    "        # 'filter' may be to exagerated of a word, but that's the idea \n",
    "        # this transforms [batch_size, sentence_length, hidden_dim] first to\n",
    "        # [batch_size, sentence_length, len(actions)] by multiplication and then\n",
    "        # to [batch_size, sentence_length] by summation\n",
    "        filter_previous_actions = (lstm_pos_out @ actions).sum(dim=2)\n",
    "        \n",
    "        pred_pos = nn.functional.softmax(filter_previous_actions, dim=1)\n",
    "        \n",
    "        ###############  next we predict a best anchor type for each token  ################\n",
    "        lstm_typer_out, self.hidden_for_typer = self.lstm_typer(embeded, self.hidden_for_typer)\n",
    "        \n",
    "        # notice that here we apply softmax over third dimention. This way we normalized \n",
    "        # pseudo-probabilities over anchor_types and over sentence lenght (as was the case \n",
    "        # when we were predicting position of anchor)\n",
    "        pred_types = nn.functional.softmax(self.linear_over_typer(lstm_typer_out), dim=2)\n",
    "        \n",
    "        return (pred_types.permute(0, 2, 1) * pred_pos).permute(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def encode_actions(self, batched_actions):\n",
    "        \"\"\"\n",
    "        Actions a torch 3d numpy array with shape [batch_size, length_of_history, 2]. \n",
    "        where *length_of_history* is how many actions were taken previously\n",
    "        Each action is a pair of integer numbers (anchor_position, anchor_type)\n",
    "        This returns a numpy float array of shape [batch_size, hidden_dim, number_of_actions = len(actions)]\n",
    "        \"\"\"\n",
    "#         return [np.array([np.ones(self.hidden_dim) / np.sqrt(self.hidden_dim)] + [np.hstack([self.encode_integer(action[0], self.hidden_dim // 2), \n",
    "#                     self.encode_integer(action[1], self.hidden_dim // 2)]) for action in actions]).T for actions in batched_actions]\n",
    "\n",
    "        return [np.vstack([np.ones(self.hidden_dim) / np.sqrt(self.hidden_dim)] + [np.hstack([self.encoded_integers_array[action[0]], \n",
    "                    self.encoded_integers_array[action[1]]]) for action in batched_actions[0]]).T]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode_integer(self, n, dim):\n",
    "        # this weird construction with .T.flatten() is only so that in the output array sin and cos \n",
    "        # would be one after another(sin(a/i*x), cos(a/i*x), sin(a/(i+1)*x), cos(a/(i+1)*x) and so on). \n",
    "        # It is not necessary, i just like it more. Simpler alternative is \n",
    "        # np.hstack([[np.sin((a/i)*n) for i in range(1, dim // 2 + 1)], [np.cos((a/i)*n) for i in range(1, dim // 2 + 1)]])\n",
    "\n",
    "        a = 2. * np.pi * np.sqrt(dim)\n",
    "        \n",
    "        return np.vstack([[np.sin((a/i)*n) for i in range(1, dim // 2 + 1)], [np.cos((a/i)*n) for i in range(1, dim // 2 + 1)]]).T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "params = filter(lambda x: x.requires_grad, policy.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    # this returns a selected action for every state in the batch.\n",
    "    # the output is a torch.LongTensor of shape [batch_size, 2], where\n",
    "    # first element is chosen anchor_position and the second - anchor_type\n",
    "    \n",
    "    prob = policy(state)\n",
    "    \n",
    "    # this creates a categorical distributions for positions for each element in batch\n",
    "    distr_pos = torch.distributions.Categorical(probs=prob.sum(dim=2))\n",
    "\n",
    "    chosen_positions = distr_pos.sample()\n",
    "\n",
    "    # here we create a distributions of anchor_types for sampled token positions in each batch\n",
    "    distr_types = torch.distributions.Categorical(probs=prob[:, chosen_positions, :].squeeze(dim=1))\n",
    "\n",
    "    chosen_types = distr_types.sample()\n",
    "\n",
    "    # log(a*b) = log(a) + log(b)\n",
    "    policy.saved_log_prob.append(distr_pos.log_prob(chosen_positions) + distr_types.log_prob(chosen_types))\n",
    "    \n",
    "    return torch.cat([chosen_positions.unsqueeze(0), chosen_types.unsqueeze(0)], dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "Epoch 0:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -89.38122593377182\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.10, accuracy of type = 0.35\n",
      "Recall of position = 0.09, recall of type = 0.29\n",
      "F1 score of position = 0.09, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 1:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -6.327738060695444\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.13, accuracy of type = 0.35\n",
      "Recall of position = 0.11, recall of type = 0.29\n",
      "F1 score of position = 0.11, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 2:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -4.032647921997477\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.16, accuracy of type = 0.35\n",
      "Recall of position = 0.14, recall of type = 0.29\n",
      "F1 score of position = 0.14, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 3:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -3.8149644111247842\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.08, accuracy of type = 0.35\n",
      "Recall of position = 0.08, recall of type = 0.29\n",
      "F1 score of position = 0.08, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 4:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -3.1864380717975913\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.06, accuracy of type = 0.35\n",
      "Recall of position = 0.06, recall of type = 0.29\n",
      "F1 score of position = 0.05, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 5:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -2.9240262090874176\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.06, accuracy of type = 0.35\n",
      "Recall of position = 0.06, recall of type = 0.29\n",
      "F1 score of position = 0.05, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 6:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -3.5659581405190997\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.11, accuracy of type = 0.35\n",
      "Recall of position = 0.16, recall of type = 0.29\n",
      "F1 score of position = 0.13, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 7:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -2.4289133441925648\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.11, accuracy of type = 0.35\n",
      "Recall of position = 0.13, recall of type = 0.29\n",
      "F1 score of position = 0.11, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 8:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -3.5068733064463724\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.14, accuracy of type = 0.35\n",
      "Recall of position = 0.18, recall of type = 0.29\n",
      "F1 score of position = 0.15, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 9:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -2.1251974614266524\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.09, accuracy of type = 0.35\n",
      "Recall of position = 0.10, recall of type = 0.29\n",
      "F1 score of position = 0.09, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 10:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -3.558407017452339\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.16, accuracy of type = 0.34\n",
      "Recall of position = 0.22, recall of type = 0.29\n",
      "F1 score of position = 0.18, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 11:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -2.02521113723275\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.16, accuracy of type = 0.35\n",
      "Recall of position = 0.24, recall of type = 0.29\n",
      "F1 score of position = 0.18, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 12:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -1.5520428917215587\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.20, accuracy of type = 0.35\n",
      "Recall of position = 0.29, recall of type = 0.29\n",
      "F1 score of position = 0.23, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 13:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -4.41037354177091\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.18, accuracy of type = 0.34\n",
      "Recall of position = 0.27, recall of type = 0.29\n",
      "F1 score of position = 0.21, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 14:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -2.7557624997643373\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.16, accuracy of type = 0.35\n",
      "Recall of position = 0.24, recall of type = 0.29\n",
      "F1 score of position = 0.18, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 15:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -1.3651118212354436\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.20, accuracy of type = 0.35\n",
      "Recall of position = 0.29, recall of type = 0.29\n",
      "F1 score of position = 0.23, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 16:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -0.970818337036908\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.19, accuracy of type = 0.35\n",
      "Recall of position = 0.26, recall of type = 0.29\n",
      "F1 score of position = 0.21, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 17:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -1.1853760455572226\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.23, accuracy of type = 0.35\n",
      "Recall of position = 0.34, recall of type = 0.29\n",
      "F1 score of position = 0.26, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 18:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -0.5243728958278137\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.20, accuracy of type = 0.34\n",
      "Recall of position = 0.29, recall of type = 0.29\n",
      "F1 score of position = 0.23, F1 of type = 0.31\n",
      "==========================================================================\n",
      "Epoch 19:\n",
      "Iterating over training dataset...finished\n",
      " Mean reward over training dataset = -1.6052171649944356\n",
      "\n",
      "Scoring test dataset...scored\n",
      "Accuracy of position = 0.22, accuracy of type = 0.34\n",
      "Recall of position = 0.32, recall of type = 0.29\n",
      "F1 score of position = 0.25, F1 of type = 0.31\n"
     ]
    }
   ],
   "source": [
    "# i don't yet know how to deal with batch processing when it is unknown when cicle will be terminated\n",
    "# (what to do if one element in batch terminated, but others are not yet finished?)\n",
    "# so for now this is the first place that will not work unless batch_size=1\n",
    "\n",
    "alpha_pos = 1.\n",
    "alpha_pos_miss = 0.5\n",
    "alpha_type = 1.\n",
    "alpha_additional_events = 0.2\n",
    "alpha_not_enought_events = 5\n",
    "\n",
    "# we will calculate reward in the foolwing way:\n",
    "# if a chosen position is in sample['anchors_position'] we add alpha_pos\n",
    "# if a chosen position is not in there, we will subtract alpha_pos_miss * np.min(np.abs(pos - real_pos))\n",
    "# (which is distance to a closest anchor position)\n",
    "# if a chosen anchor type is in sample['anchors_types'], we add alpha_type \n",
    "\n",
    "# if after a termination we found more (or less) events than in ground truth, \n",
    "# we subtract alpha_number_of_events * (real_number_of_events - i)**2 from every i-th event that was \n",
    "# predicted after real_number_of_events (except the last one) and subtracts alpha_not_enought_events * np.abs(real_number_of_events - i)\n",
    "# from the premature termination step\n",
    "\n",
    "def get_imediate_reward(action, sample):\n",
    "    # remember that one action is a LongTensor with shape [batch_size, 1, 2]\n",
    "    reward_ = [0] * policy.batch_size # there will be batch_size number of rewards\n",
    "    \n",
    "#     for i in range(policy.batch_size):\n",
    "#         if(action[i, 0, 0].item() in sample['anchors_position'][i]):\n",
    "#             reward_[i] += alpha_pos\n",
    "#         else:\n",
    "#             reward_[i] -= alpha_pos_miss * np.min(np.abs(sample['anchors_position'][i] - action[i, 0, 0].item()))\n",
    "\n",
    "#         if(action[i, 0, 1].item() in sample['anchors_types'][i]):\n",
    "#             reward_[i] += alpha_type\n",
    "    \n",
    "    if(action[0, 0, 0].item() in sample['anchors_position'][i]):\n",
    "        reward_[0] += alpha_pos\n",
    "    else:\n",
    "        reward_[0] -= alpha_pos_miss * np.min(np.abs(sample['anchors_position'][i] - action[0, 0, 0].item()))\n",
    "\n",
    "    if(action[0, 0, 1].item() in sample['anchors_types'][i]):\n",
    "\n",
    "    \n",
    "    return np.array(reward_)\n",
    "           \n",
    "\n",
    "def iterate_training_sample(sample):\n",
    "    reward = []\n",
    "    policy.saved_log_prob = []\n",
    "    policy.reinit_hidden()\n",
    "\n",
    "    state = (sample['input'], [[]])\n",
    "    action = select_action(state)\n",
    "    reward.append(get_imediate_reward(action, sample))\n",
    "    stop_counter = 7 # here we cheat a bit, cause in the dataset there are samples with more events than 7\n",
    "                     # but there are less then 10 of them, so it's not that big of a deal\n",
    "\n",
    "    action_history = action\n",
    "\n",
    "    while(action[0, 0, 0].is_nonzero() and stop_counter > 0):\n",
    "        stop_counter = stop_counter - 1\n",
    "\n",
    "        state = (sample['input'], action_history)\n",
    "        action = select_action(state)\n",
    "\n",
    "        reward.append(get_imediate_reward(action, sample))\n",
    "\n",
    "        action_history = torch.cat([action_history, action], dim=1)\n",
    "\n",
    "    # rigth now reward have the shape [action_history, batch_size], so we would like to \n",
    "    # reverse it for consistency\n",
    "    reward = np.array(reward).T\n",
    "\n",
    "    # don't forget - action_history is a torch tensor of shape [batch_size, history_lenght, 2]\n",
    "    # and sample['anchors_position'] is a numpy array of shape [batch_size, number_of_events_in_sentence + 1]\n",
    "    # where +1 is a termination \"event\"\n",
    "    if(action_history.shape[1] > len(sample['anchors_position'][0])):\n",
    "        overshots = reward[:, len(sample['anchors_position'][0]) - 1:-1] \n",
    "\n",
    "        overshots -= alpha_additional_events * np.array([np.arange(1, len(overshots[0]) + 1)**2 for _ in range(len(overshots))])\n",
    "\n",
    "        reward[:, len(sample['anchors_position'][0]) - 1:-1] = overshots\n",
    "    elif(action_history.shape[1] < len(sample['anchors_position'][0])):\n",
    "        reward[:, -1] -= alpha_not_enought_events * (len(sample['anchors_position'][0]) - action_history.shape[1])\n",
    "\n",
    "\n",
    "    reward = torch.FloatTensor(reward)\n",
    "    loss = -(torch.cat(policy.saved_log_prob) * reward).sum(dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.saved_log_prob[:]\n",
    "    del reward\n",
    "    \n",
    "    return torch.mean(loss.data).detach().item()\n",
    "\n",
    "\n",
    "def predict(sample):\n",
    "    state = (sample['input'], [[]])\n",
    "    \n",
    "    policy.reinit_hidden()\n",
    "    pred = policy(state)\n",
    "\n",
    "    positions = pred.sum(dim=2).argmax(dim=1)\n",
    "    types = pred[:, positions, :].argmax(dim=2)\n",
    "\n",
    "    action = torch.cat((positions.unsqueeze(0), types), dim=1).unsqueeze(1)\n",
    "    \n",
    "    action_history = action\n",
    "    stop_counter = 17\n",
    "    \n",
    "    while(action[0, 0, 0].is_nonzero() and stop_counter > 0):\n",
    "        stop_counter = stop_counter - 1\n",
    "\n",
    "        state = (sample['input'], action_history)\n",
    "        pred = policy(state)\n",
    "        positions = pred.sum(dim=2).argmax(dim=1)\n",
    "        types = pred[:, positions, :].argmax(dim=2)\n",
    "        action = torch.cat((positions.unsqueeze(0), types), dim=1).unsqueeze(1)\n",
    "\n",
    "        action_history = torch.cat([action_history, action], dim=1)\n",
    "    \n",
    "    return action_history\n",
    "    \n",
    "\n",
    "def score_prediction(action_history, sample):\n",
    "    if(action_history.shape[1] > 1):\n",
    "        pred_types = action_history[:, :-1, 1].numpy()\n",
    "        real_types = np.array(sample['anchors_types'])[:, :-1]\n",
    "    else:\n",
    "        pred_types = action_history[:, :, 1].numpy()\n",
    "        real_types = np.array(sample['anchors_types'])[:, :-1]\n",
    "    \n",
    "    recall_data_type = np.array([[event in pred_pos_ for event in real_pos_] for pred_pos_, real_pos_ in zip(pred_types, real_types)])\n",
    "    accuracy_data_type = np.array([[event in real_pos_ for event in pred_pos_] for pred_pos_, real_pos_ in zip(pred_types, real_types)])\n",
    "\n",
    "    mean_recall_type = (recall_data_type.sum(axis=1) / recall_data_type.shape[1]).mean()\n",
    "    mean_accuracy_type = (accuracy_data_type.sum(axis=1) / accuracy_data_type.shape[1]).mean()\n",
    "\n",
    "    f1_type = 2.* mean_recall_type * mean_accuracy_type / (mean_recall_type + mean_accuracy_type + 1e-7)\n",
    "\n",
    "    if(action_history.shape[1] == 1):\n",
    "        return 0., mean_accuracy_type, 0., mean_recall_type, 0., f1_type\n",
    "    else:\n",
    "        pred_pos = action_history[:, :-1, 0].numpy()\n",
    "        real_pos = np.array(sample['anchors_position'])[:, :-1]\n",
    "\n",
    "    recall_data_pos = np.array([[event in pred_pos_ for event in real_pos_] for pred_pos_, real_pos_ in zip(pred_pos, real_pos)])\n",
    "    accuracy_data_pos = np.array([[event in real_pos_ for event in pred_pos_] for pred_pos_, real_pos_ in zip(pred_pos, real_pos)])\n",
    "\n",
    "    mean_recall_pos = (recall_data_pos.sum(axis=1) / recall_data_pos.shape[1]).mean()\n",
    "    mean_accuracy_pos = (accuracy_data_pos.sum(axis=1) / accuracy_data_pos.shape[1]).mean()\n",
    "\n",
    "    f1_pos = 2.* mean_recall_pos * mean_accuracy_pos / (mean_recall_pos + mean_accuracy_pos + 1e-7)\n",
    "\n",
    "    return mean_accuracy_pos, mean_accuracy_type, mean_recall_pos, mean_recall_type, f1_pos, f1_type\n",
    "\n",
    "\n",
    "EPOCH = 20    \n",
    "\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.5)\n",
    "\n",
    "reward_history = []\n",
    "stats_history = []\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('==========================================================================')\n",
    "    print('Epoch {}:'.format(i))\n",
    "    \n",
    "    dataset_train = shuffle(dataset_train)\n",
    "    \n",
    "    reward_over_training_dataset = 0\n",
    "    \n",
    "    print('Iterating over training dataset...', end='')\n",
    "    for sample in dataset_train:\n",
    "        \n",
    "        reward_over_training_dataset += iterate_training_sample(sample)\n",
    "    \n",
    "    reward_history.append(reward_over_training_dataset)\n",
    "    print('finished\\n Mean reward over training dataset = {}'.format(reward_over_training_dataset / len(dataset_train)))\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    \n",
    "    print('\\nScoring test dataset...', end='')\n",
    "    \n",
    "    for sample in dataset_test:\n",
    "        \n",
    "        predicted_actions = predict(sample)\n",
    "        \n",
    "        mean_accuracy_pos, mean_accuracy_type, mean_recall_pos, mean_recall_type, f1_pos, f1_type = score_prediction(predicted_actions, sample)\n",
    "        \n",
    "        accuracy.append((mean_accuracy_pos, mean_accuracy_type))\n",
    "        recall.append((mean_recall_pos, mean_recall_type))\n",
    "        f1.append((f1_pos, f1_type))\n",
    "    \n",
    "    recall = np.array(recall).mean(axis=0)\n",
    "    accuracy = np.array(accuracy).mean(axis=0)\n",
    "    f1 = np.array(f1).mean(axis=0)\n",
    "    \n",
    "    stats_history.append(np.hstack([accuracy, recall, f1]))\n",
    "    \n",
    "    print(\"scored\")\n",
    "    print('Accuracy of position = {:.2f}, accuracy of type = {:.2f}'.format(accuracy[0], accuracy[1]))\n",
    "    print('Recall of position = {:.2f}, recall of type = {:.2f}'.format(recall[0], recall[1]))\n",
    "    print('F1 score of position = {:.2f}, F1 of type = {:.2f}'.format(f1[0], f1[1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([(1, 1), (2, 5), (-1, 3)]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0d1b4b53d848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maction_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'action_history' is not defined"
     ]
    }
   ],
   "source": [
    "action_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(dataset_test[0])[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]), array([[4]]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(pred, real):\n",
    "    # [bacth_size, prediction_length, 2] and [batch_size, number_of_events, 2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if((i[:, :, 0] > 0).all()):\n",
    "    print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy.reinit_hidden()\n",
    "\n",
    "action = select_action(state)\n",
    "\n",
    "stop_counter = 17\n",
    "\n",
    "action_history = action\n",
    "\n",
    "while(action[0, 0, 0].is_nonzero() and stop_counter > 0):\n",
    "    stop_counter = stop_counter - 1\n",
    "\n",
    "    state = (sample['input'], action_history)\n",
    "    action = select_action(state)\n",
    "\n",
    "    reward.append(get_imediate_reward(action, sample))\n",
    "\n",
    "    action_history = torch.cat([action_history, action], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean is not implemented for type torch.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-4b869b66f2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mean is not implemented for type torch.LongTensor"
     ]
    }
   ],
   "source": [
    "torch.mean(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[]]\n",
    "\n",
    "a3 = select_action((dataset[0]['input'], torch.cat([a2, a1], dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 40,   4]]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 50 and 2 in dimension 2 at /opt/conda/conda-bld/pytorch-cpu_1524582300956/work/aten/src/TH/generic/THTensorMath.c:3586",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-68e335395cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 50 and 2 in dimension 2 at /opt/conda/conda-bld/pytorch-cpu_1524582300956/work/aten/src/TH/generic/THTensorMath.c:3586"
     ]
    }
   ],
   "source": [
    "torch.cat([torch.FloatTensor(a), a1.float()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3, 4)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  1,  3])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.22e+03, 8.74e+02, 2.02e+02, 5.90e+01, 2.00e+01, 2.00e+00,\n",
       "        1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 2.00e+00]),\n",
       " array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14., 15., 16.]),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see how many samples have more than one event:\n",
    "number_of_events = [len(sample['anchors_types']) for sample in dataset]\n",
    "\n",
    "# wow, there are even sentences with 16 events!\n",
    "plt.hist(number_of_events, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now i do not imlement batch processing, since it is unlikely to necessary for such a small dataset\n",
    "\n",
    "# def group_dataset (dataset):\n",
    "#     # we would like to pad our sentences to the same lenght (for batching), but if were to pad them to\n",
    "#     # maximum length, they all will be extreamly long (~160 tokens) and it will slow down computation\n",
    "#     # even with batching. So we will split them in several groups\n",
    "\n",
    "#     lenghts = [len(sample['input']) for sample in dataset]\n",
    "    \n",
    "#     # the borders are picked more-or-less empiricaly so that the group sizes are aproximately equal\n",
    "#     group1 = np.array(lenghts) < 16\n",
    "#     group2 = np.logical_and(np.array(lenghts) >= 16, np.array(lenghts) < 23)\n",
    "#     group3 = np.logical_and(np.array(lenghts) >= 23, np.array(lenghts) < 32)\n",
    "#     group4 = np.array(lenghts) >= 32\n",
    "#     groups = [group1, group2, group3, group4]\n",
    "    \n",
    "#     new_dataset = []\n",
    "    \n",
    "#     for group in groups:\n",
    "#         new_dataset.append(np.array(dataset)[group])\n",
    "    \n",
    "#     return new_dataset\n",
    "\n",
    "# def iterate_batches (dataset):\n",
    "#     pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventPointerNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 data_file_name = 'data.json', \n",
    "                 embedding_dim=100, \n",
    "                 hidden_dim = 100, \n",
    "                 batch_size=1, \n",
    "                 lstm_layers=1,\n",
    "                 mode='train_word2vec',\n",
    "                 trainable_embedding = False,\n",
    "                 use_unknown_token=False,\n",
    "                 added_noise_varience=0.1):\n",
    "        \n",
    "        super(EventPointerNetwork, self).__init__()\n",
    "        \n",
    "        self.data_file_name = data_file_name\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.rotation = Variable(torch.FloatTensor(ortho_group.rvs(dim=100, size=self.batch_size)), requires_grad=False)\n",
    "        self.added_noise_varience = added_noise_varience\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim // 2,  # because bidirectional\n",
    "                            num_layers=lstm_layers,\n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size=batch_size)\n",
    "        \n",
    "        # this layer will transform [sequence_lenght, hidden_dim] to [sequence_lenght, 1] so that we can apply softmax\n",
    "        # and get a word in sequance which is the most likely \"action anchor\"\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.LoadAndTokenizeData(mode='spacy')\n",
    "        self.InitEmbeddingAndCreateDataset(mode=mode, trainable = trainable_embedding, use_unknown_token=use_unknown_token)\n",
    "    \n",
    "    def LoadAndTokenizeData(self, mode='spacy'):\n",
    "        \"\"\"This function will load the data fron json file self.data_file_name. Then it will tokenize it\n",
    "           using either spacy parser or simple splitting. Tokenized data will be saved in self.tokenized_lines\n",
    "           \n",
    "           INPUT:\n",
    "               mode - string. Either 'spacy' or 'simple_split'\n",
    "        \"\"\"\n",
    "        # first we will load the data\n",
    "        data = json.load(open(self.data_file_name))\n",
    "        \n",
    "        # now we extract sentences with events in them \n",
    "        # (we are implicitly assuming here that we can exctract events without looking at sentences before or after the one \n",
    "        # with the event in it)\n",
    "        lines = []\n",
    "        anchor_types = [] \n",
    "        self.anchor_text = [] # both lines and anchor_types we do not need outside this function, but anchor_text we will need\n",
    "                              # to save untill creation of token vocabulary\n",
    "\n",
    "        for text in data:\n",
    "            for event in text['events']:\n",
    "                for mention in event['MENTIONS']:\n",
    "                    lines.append(mention['scope_text'])\n",
    "                    anchor_types.append(event['TYPE'])\n",
    "                    self.anchor_text.append(mention['anchor_text'])\n",
    "    \n",
    "        # parse and tokenize. TODO: create vocab out of all text, not only with events\n",
    "        if(mode == 'spacy'): # using sapcy english parser with fancy rules\n",
    "            parser = English()\n",
    "            self.tokenized_lines = [[token.text for token in parser(line.replace('\\n', ' '))] for line in lines]\n",
    "            del parser \n",
    "        elif(mode == 'simple_split'):  # or just splitting sentences as is\n",
    "            self.tokenized_lines = [line.replace('\\n', ' ').split() for line in lines]\n",
    "    \n",
    "        # assign each event name a unique number\n",
    "        anchor_type_to_id = {name:i for name, i in zip(np.unique(anchor_types), range(len(np.unique(anchor_types))))}\n",
    "        self.anchor_labels = [anchor_type_to_id[anchor_type] for anchor_type in anchor_types]\n",
    "        \n",
    "    \n",
    "    def InitEmbeddingAndCreateDataset(self, \n",
    "                                      mode='train_word2vec',\n",
    "                                      trainable = False,\n",
    "                                      use_unknown_token=False, \n",
    "                                      min_count=3, window=5, test_ratio=0.2):\n",
    "        \n",
    "        if(mode == 'random'):\n",
    "            pass # will do later\n",
    "        elif(mode == 'train_word2vec'):\n",
    "            if(use_unknown_token):\n",
    "                self.tokenized_lines.append(['_UNKNOWN_'] * min_count)\n",
    "            model = Word2Vec(sentences=self.tokenized_lines, \n",
    "                             size=self.embedding_dim, \n",
    "                             window=5, \n",
    "                             min_count=min_count, \n",
    "                             workers=2, sg=0)\n",
    "            \n",
    "            filtered_tokenized_lines = []\n",
    "            \n",
    "            if(use_unknown_token):\n",
    "                for line in self.tokenized_lines:\n",
    "                    filtered_tokenized_lines.append([token if token in model.wv.vocab else '_UNKNOWN_' for token in line])\n",
    "            else:\n",
    "                for line in self.tokenized_lines:\n",
    "                    filtered_tokenized_lines.append([token for token in line if(token in model.wv.vocab)])\n",
    "                    \n",
    "            word2id = {word:model.wv.index2word.index(word) for word in model.wv.index2word}\n",
    "        \n",
    "            self.embeddings = nn.Embedding(len(model.wv.vectors), self.embedding_dim)\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(model.wv.vectors))\n",
    "            if(not trainable):\n",
    "                self.embeddings.weight.requires_grad = False\n",
    "        \n",
    "        dataset = []\n",
    "        for i in range(len(filtered_tokenized_lines) - int(use_unknown_token)):\n",
    "            \n",
    "            # if anchor word does not appear in dictionary, we skipp this sample (otherwise \n",
    "            # we will be trying to point to a word we can't see, which is pointles)\n",
    "            if(self.anchor_text[i] in filtered_tokenized_lines[i]):\n",
    "                Anchor_position = filtered_tokenized_lines[i].index(self.anchor_text[i])\n",
    "            else: \n",
    "                continue\n",
    "            \n",
    "            Input = Variable(torch.LongTensor([word2id[word] for word in filtered_tokenized_lines[i]]),\n",
    "                             requires_grad=False)\n",
    "            \n",
    "            data_point = {'Input':Input.resize(1, len(Input)),  #no batches for now\n",
    "                          'Anchor position':Variable(torch.LongTensor([Anchor_position]), requires_grad=False),\n",
    "                          'Anchor label':Variable(torch.LongTensor([self.anchor_labels[i]]), requires_grad=False)}\n",
    "            \n",
    "            dataset.append(data_point)\n",
    "        \n",
    "        self.train_dataset, self.test_dataset = train_test_split(dataset, test_size=test_ratio)\n",
    "            \n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (Variable(torch.zeros(self.lstm_layers * 2, batch_size, self.hidden_dim // 2)),\n",
    "                Variable(torch.zeros(self.lstm_layers * 2, batch_size, self.hidden_dim // 2)))\n",
    "    \n",
    "    def train_(self, epochs = 20, lr=1e-3, wd=1e-4, augmentation=False):\n",
    "        \n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        parameters = filter(lambda x: x.requires_grad, self.parameters())\n",
    "        optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "        \n",
    "        train_losses = []\n",
    "        \n",
    "        for i in tqdm.tqdm_notebook(range(epochs)):\n",
    "            \n",
    "            train_losses.append([0])\n",
    "            \n",
    "            # switch to train mode \n",
    "            self.train()\n",
    "            \n",
    "            for sample in self.train_dataset:\n",
    "                if(augmentation):\n",
    "                    self.hidden = self.init_hidden(batch_size=self.batch_size)\n",
    "                else:\n",
    "                    self.hidden = self.init_hidden()\n",
    "                \n",
    "                pred = self.forward(sample['Input'], augmentation=augmentation)\n",
    "                \n",
    "                loss = loss_function(pred, sample['Anchor position'])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_losses[-1].append(loss.data[0])\n",
    "            \n",
    "            # switch to evaluation mode (switch off dropout and batch norm)\n",
    "            self.eval()\n",
    "            \n",
    "            pred_correctly = 0\n",
    "            \n",
    "            for sample in self.test_dataset:\n",
    "                self.hidden = self.init_hidden()\n",
    "                \n",
    "                pred = self.forward(sample['Input'])\n",
    "                \n",
    "                _, collased_pred = torch.max(pred.data, 1)\n",
    "                \n",
    "                if(collased_pred[0] == sample['Anchor position'].data[0]):\n",
    "                    pred_correctly += 1\n",
    "                \n",
    "            acc = float(pred_correctly) / len(self.test_dataset)\n",
    "            \n",
    "            print('Epoch {}. Train loss: {:.2f}. Test accuracy: {:.2f}'.format(i, np.sum(train_losses[-1]), acc))\n",
    "            \n",
    "    def forward(self, Input, augmentation=False):\n",
    "        embeded = self.embeddings(Input)\n",
    "        \n",
    "        if(augmentation):\n",
    "            embeded = embeded * Variable(1 + self.added_noise_varience*torch.randn((self.batch_size, 1, self.embedding_dim)),\n",
    "                                        requires_grad = False)\n",
    "        \n",
    "        lstm_output, self.hidden = self.lstm(embeded.permute(1, 0, 2), self.hidden)\n",
    "        \n",
    "        return nn.functional.softmax(\n",
    "            self.linear(lstm_output.resize(lstm_output.size()[0] * lstm_output.size()[1], lstm_output.size()[2])), dim=0).permute(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EventPointerNetwork(trainable_embedding = True, use_unknown_token=False, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 ms ± 547 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.forward(model.train_dataset[0]['Input'], augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71e2c081dda412dba8bf9104be9e98f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 12762.50. Test accuracy: 0.13, recall: 0.13, precision: 0.13, F1: 0.13\n",
      "Epoch 1. Train loss: 12661.60. Test accuracy: 0.13, recall: 0.13, precision: 0.13, F1: 0.13\n",
      "Epoch 2. Train loss: 12636.79. Test accuracy: 0.12, recall: 0.12, precision: 0.12, F1: 0.12\n",
      "Epoch 3. Train loss: 12605.39. Test accuracy: 0.13, recall: 0.13, precision: 0.13, F1: 0.13\n",
      "Epoch 4. Train loss: 12563.50. Test accuracy: 0.15, recall: 0.15, precision: 0.15, F1: 0.15\n",
      "Epoch 5. Train loss: 12530.89. Test accuracy: 0.16, recall: 0.16, precision: 0.16, F1: 0.16\n",
      "Epoch 6. Train loss: 12472.59. Test accuracy: 0.17, recall: 0.17, precision: 0.17, F1: 0.17\n",
      "Epoch 7. Train loss: 12400.89. Test accuracy: 0.24, recall: 0.24, precision: 0.24, F1: 0.24\n",
      "Epoch 8. Train loss: 12313.11. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 9. Train loss: 12280.85. Test accuracy: 0.26, recall: 0.26, precision: 0.26, F1: 0.26\n",
      "Epoch 10. Train loss: 12245.61. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 11. Train loss: 12236.90. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 12. Train loss: 12215.30. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 13. Train loss: 12194.59. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 14. Train loss: 12178.83. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 15. Train loss: 12170.14. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 16. Train loss: 12158.40. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 17. Train loss: 12151.30. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 18. Train loss: 12140.09. Test accuracy: 0.27, recall: 0.27, precision: 0.27, F1: 0.27\n",
      "Epoch 19. Train loss: 12135.78. Test accuracy: 0.28, recall: 0.28, precision: 0.28, F1: 0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = EventPointerNetwork(trainable_embedding = False, use_unknown_token=False)\n",
    "model.train_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaca04961adb4da7b0c71d5757b1dd9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 11356.69. Test accuracy: 0.58, recall: 0.58, precision: 0.58, F1: 0.58\n",
      "Epoch 1. Train loss: 10826.73. Test accuracy: 0.58, recall: 0.58, precision: 0.58, F1: 0.58\n",
      "Epoch 2. Train loss: 10760.07. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 3. Train loss: 10700.07. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "Epoch 4. Train loss: 10660.80. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 5. Train loss: 10617.44. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 6. Train loss: 10610.16. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 7. Train loss: 10603.81. Test accuracy: 0.61, recall: 0.61, precision: 0.61, F1: 0.61\n",
      "Epoch 8. Train loss: 10572.47. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "Epoch 9. Train loss: 10534.71. Test accuracy: 0.61, recall: 0.61, precision: 0.61, F1: 0.61\n",
      "Epoch 10. Train loss: 10531.27. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "Epoch 11. Train loss: 10500.74. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 12. Train loss: 10474.39. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "Epoch 13. Train loss: 10468.35. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 14. Train loss: 10448.90. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "Epoch 15. Train loss: 10454.07. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 16. Train loss: 10439.18. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 17. Train loss: 10420.94. Test accuracy: 0.60, recall: 0.60, precision: 0.60, F1: 0.60\n",
      "Epoch 18. Train loss: 10450.39. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "Epoch 19. Train loss: 10390.05. Test accuracy: 0.59, recall: 0.59, precision: 0.59, F1: 0.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = EventPointerNetwork(trainable_embedding = True, use_unknown_token=False)\n",
    "model.train_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0e3d26f83546b691310600e3832adb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 12967.81. Test accuracy: 0.16, recall: 0.16, precision: 0.16, F1: 0.16\n",
      "Epoch 1. Train loss: 12891.55. Test accuracy: 0.18, recall: 0.18, precision: 0.18, F1: 0.18\n",
      "Epoch 2. Train loss: 12862.82. Test accuracy: 0.14, recall: 0.14, precision: 0.14, F1: 0.14\n",
      "Epoch 3. Train loss: 12809.88. Test accuracy: 0.19, recall: 0.19, precision: 0.19, F1: 0.19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-826fc9d71164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventPointerNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unknown_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5f5fdcd90819>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, epochs, lr, wd)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollased_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5f5fdcd90819>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Input)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0membeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         return nn.functional.softmax(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         )\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EventPointerNetwork(trainable_embedding = False, use_unknown_token=True)\n",
    "model.train_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828da1e04be74d0288e9b504623c8d62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/student/miniconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/student/miniconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 22542.93. Test accuracy: 0.01\n",
      "Epoch 1. Train loss: 22538.52. Test accuracy: 0.01\n",
      "Epoch 2. Train loss: 22538.46. Test accuracy: 0.01\n",
      "Epoch 3. Train loss: 22538.46. Test accuracy: 0.01\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f92f2da024a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventPointerNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unknown_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madded_noise_varience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-e5f20bbff060>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, epochs, lr, wd, augmentation)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Anchor position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e5f20bbff060>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Input, augmentation)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         return nn.functional.softmax(\n\u001b[0;32m--> 195\u001b[0;31m             self.linear(lstm_output.resize(lstm_output.size()[0] * lstm_output.size()[1], lstm_output.size()[2])), dim=0).permute(1, 0)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EventPointerNetwork(trainable_embedding = True, use_unknown_token=False, batch_size=10, added_noise_varience=0.01)\n",
    "model.train_(augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.107873  , -0.14772108, -0.00431708, ...,  0.00620962,\n",
       "        -0.02809927, -0.03568291],\n",
       "       [-0.02488238,  0.06697029, -0.08345047, ...,  0.02791858,\n",
       "        -0.00062978,  0.03429892],\n",
       "       [ 0.02105349,  0.00685935,  0.12863055, ..., -0.16188012,\n",
       "         0.07248722, -0.03051877],\n",
       "       ...,\n",
       "       [ 0.04766418,  0.14956476,  0.19661506, ..., -0.07164895,\n",
       "         0.00184981,  0.17163976],\n",
       "       [ 0.18516098,  0.18092651, -0.08862681, ..., -0.13351746,\n",
       "        -0.04337998, -0.02686833],\n",
       "       [-0.09572826,  0.00231094,  0.10542071, ..., -0.16580989,\n",
       "        -0.25362044,  0.16321052]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ortho_group.rvs(dim=100, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.embeddings(model.train_dataset[0]['Input']).data[0, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00639549,  0.02478294, -0.03556288,  0.02758429, -0.03052169,\n",
       "        0.05740496, -0.05428189, -0.04999069,  0.02229912,  0.00859046,\n",
       "       -0.00764467,  0.06781983, -0.05810191, -0.01523107,  0.04677327,\n",
       "       -0.03403283,  0.04241906,  0.07917542,  0.02908635, -0.02563982,\n",
       "        0.02447637, -0.00227952,  0.08291341,  0.01861861, -0.01903705,\n",
       "       -0.00591173, -0.01842427,  0.01474347,  0.04095296,  0.00216009,\n",
       "       -0.0174778 , -0.01352988,  0.07130295,  0.00138191,  0.01719273,\n",
       "       -0.01081256, -0.00988321,  0.00543546,  0.06191076, -0.0011535 ,\n",
       "       -0.03407233,  0.01007321, -0.00492583,  0.02421363, -0.01700173,\n",
       "       -0.02944059,  0.01196152, -0.02628855,  0.03551354, -0.07877893,\n",
       "       -0.04407536,  0.01080482,  0.01933007,  0.00683997,  0.01266157,\n",
       "        0.05913918, -0.0517735 ,  0.02160677,  0.01413801,  0.06702278,\n",
       "        0.03176447,  0.02949127, -0.0087865 ,  0.02734794,  0.03339221,\n",
       "       -0.03924424,  0.05853817, -0.01468915,  0.00685779,  0.01642132,\n",
       "        0.06710401, -0.00480234,  0.00817485,  0.0422732 ,  0.03504913,\n",
       "       -0.00787641,  0.01000546,  0.05362109,  0.03625699, -0.06108852,\n",
       "        0.03498304, -0.07617662, -0.0490495 ,  0.04351851, -0.08425058,\n",
       "       -0.05501775,  0.01143333, -0.02418513, -0.02612466, -0.05768779,\n",
       "        0.02013057, -0.02258212,  0.03260576,  0.02244915, -0.04967006,\n",
       "        0.00578417,  0.04699384, -0.08912631, -0.01562339,  0.00275017])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv @ ortho_group.rvs(dim=100, size=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.0064  0.0248 -0.0356  ...  -0.0891 -0.0156  0.0028\n",
       "  0.0002  0.0176 -0.0338  ...  -0.0679 -0.0139 -0.0027\n",
       "  0.0815  0.4256 -0.5050  ...  -1.1638 -0.0886 -0.1622\n",
       "           ...             ⋱             ...          \n",
       "  0.0305  0.0780 -0.1618  ...  -0.3365 -0.0756 -0.0189\n",
       "  0.1230  0.2651 -0.5535  ...  -1.1788 -0.2414 -0.0128\n",
       "  0.0095  0.0148 -0.0301  ...  -0.0672 -0.0188 -0.0024\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  0.0064  0.0248 -0.0356  ...  -0.0891 -0.0156  0.0028\n",
       "  0.0002  0.0176 -0.0338  ...  -0.0679 -0.0139 -0.0027\n",
       "  0.0815  0.4256 -0.5050  ...  -1.1638 -0.0886 -0.1622\n",
       "           ...             ⋱             ...          \n",
       "  0.0305  0.0780 -0.1618  ...  -0.3365 -0.0756 -0.0189\n",
       "  0.1230  0.2651 -0.5535  ...  -1.1788 -0.2414 -0.0128\n",
       "  0.0095  0.0148 -0.0301  ...  -0.0672 -0.0188 -0.0024\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  0.0064  0.0248 -0.0356  ...  -0.0891 -0.0156  0.0028\n",
       "  0.0002  0.0176 -0.0338  ...  -0.0679 -0.0139 -0.0027\n",
       "  0.0815  0.4256 -0.5050  ...  -1.1638 -0.0886 -0.1622\n",
       "           ...             ⋱             ...          \n",
       "  0.0305  0.0780 -0.1618  ...  -0.3365 -0.0756 -0.0189\n",
       "  0.1230  0.2651 -0.5535  ...  -1.1788 -0.2414 -0.0128\n",
       "  0.0095  0.0148 -0.0301  ...  -0.0672 -0.0188 -0.0024\n",
       "... \n",
       "\n",
       "( 6 ,.,.) = \n",
       "  0.0064  0.0248 -0.0356  ...  -0.0891 -0.0156  0.0028\n",
       "  0.0002  0.0176 -0.0338  ...  -0.0679 -0.0139 -0.0027\n",
       "  0.0815  0.4256 -0.5050  ...  -1.1638 -0.0886 -0.1622\n",
       "           ...             ⋱             ...          \n",
       "  0.0305  0.0780 -0.1618  ...  -0.3365 -0.0756 -0.0189\n",
       "  0.1230  0.2651 -0.5535  ...  -1.1788 -0.2414 -0.0128\n",
       "  0.0095  0.0148 -0.0301  ...  -0.0672 -0.0188 -0.0024\n",
       "\n",
       "( 7 ,.,.) = \n",
       "  0.0064  0.0248 -0.0356  ...  -0.0891 -0.0156  0.0028\n",
       "  0.0002  0.0176 -0.0338  ...  -0.0679 -0.0139 -0.0027\n",
       "  0.0815  0.4256 -0.5050  ...  -1.1638 -0.0886 -0.1622\n",
       "           ...             ⋱             ...          \n",
       "  0.0305  0.0780 -0.1618  ...  -0.3365 -0.0756 -0.0189\n",
       "  0.1230  0.2651 -0.5535  ...  -1.1788 -0.2414 -0.0128\n",
       "  0.0095  0.0148 -0.0301  ...  -0.0672 -0.0188 -0.0024\n",
       "\n",
       "( 8 ,.,.) = \n",
       "  0.0064  0.0248 -0.0356  ...  -0.0891 -0.0156  0.0028\n",
       "  0.0002  0.0176 -0.0338  ...  -0.0679 -0.0139 -0.0027\n",
       "  0.0815  0.4256 -0.5050  ...  -1.1638 -0.0886 -0.1622\n",
       "           ...             ⋱             ...          \n",
       "  0.0305  0.0780 -0.1618  ...  -0.3365 -0.0756 -0.0189\n",
       "  0.1230  0.2651 -0.5535  ...  -1.1788 -0.2414 -0.0128\n",
       "  0.0095  0.0148 -0.0301  ...  -0.0672 -0.0188 -0.0024\n",
       "[torch.FloatTensor of size 9x28x100]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMBEDDING_SIZE = 100\n",
    "VOCAB_SIZE = 2000\n",
    "\n",
    "#####################################################################################\n",
    "# First we construct datasets for training 1. word embedding and 2. LSTM classifier #\n",
    "#####################################################################################\n",
    "\n",
    "# for classifier:\n",
    "lines = []\n",
    "anchor_types = []\n",
    "anchor_text = []\n",
    "\n",
    "for text in data:\n",
    "    for event in text['events']:\n",
    "        for mention in event['MENTIONS']:\n",
    "            lines.append(mention['scope_text'])\n",
    "            anchor_types.append(event['TYPE'])\n",
    "            anchor_text.append(mention['anchor_text'])\n",
    "\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    lines[i] = lines[i].lower().replace('\\n', ' ')\n",
    "    \n",
    "# dataset for training word2vec embedding:\n",
    "dataset = []\n",
    "\n",
    "for line in lines:\n",
    "    sentence = line.split()\n",
    "    for i in range(2, len(sentence) - 2):\n",
    "        context = [sentence[i - 2], sentence[i - 1], sentence[i + 1], sentence[i + 2]]\n",
    "        target = sentence[i]\n",
    "        dataset.append((context, target))    \n",
    "\n",
    "#####################################################################################\n",
    "# Now lets create a way to translate words into their id's                          #\n",
    "#####################################################################################\n",
    "\n",
    "words = dict()\n",
    "\n",
    "# here we calculate frequency of each word (every word with low enough frequancy will be considered UNKNOWN and \n",
    "# ebmbeded with the same vector)\n",
    "for line in lines:\n",
    "    for word in line.split():\n",
    "        if(words.get(word) == None):\n",
    "            words[word] = 1\n",
    "        else: words[word] += 1\n",
    "\n",
    "            \n",
    "# we take (VOCAB_SIZE - 1) most frequantly encountered words. -1 for UNKNOWN:\n",
    "words_to_encode = sorted(words, key=words.get)[::-1][:VOCAB_SIZE - 1]\n",
    "\n",
    "# dictionary to \"translate\" word to unique number\n",
    "word_to_ix = dict()\n",
    "\n",
    "for i in range(len(words_to_encode)):\n",
    "    word_to_ix[words_to_encode[i]] = i\n",
    "\n",
    "# however, we will need to \"wrap\" this dictionary in function so that every UNKNOWN word will produce (VOCAB_SIZE - 1) label\n",
    "def word_to_index(word):\n",
    "    if(word_to_ix.get(word) != None):\n",
    "        return word_to_ix[word]\n",
    "    else: return (VOCAB_SIZE - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# A class for NN with one layer for training word2vec embeding                      #\n",
    "#####################################################################################\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, context_size=2, embedding_size=100, vocab_size=2000):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, max_norm=embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def GetEmbedding(self, input_word_index):\n",
    "        return self.embeddings(autograd.Variable(torch.LongTensor(input_word_index), requires_grad=False)).data\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        lookup_embeds = self.embeddings(inputs)\n",
    "        embeds = lookup_embeds.sum(dim=0)\n",
    "        out = self.linear1(embeds)\n",
    "        out = nn.functional.log_softmax(out, dim=0)\n",
    "        return out.view(1, self.vocab_size)\n",
    "\n",
    "#####################################################################################\n",
    "# And, finaly, the training itself                                                  #\n",
    "#####################################################################################\n",
    "    \n",
    "w2v = CBOW(CONTEXT_SIZE, EMBEDDING_SIZE, VOCAB_SIZE)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "regularization = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(w2v.parameters(), lr=0.01, momentum=0.1, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "\n",
    "start_time = time.time()\n",
    "total_loss = [0]\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    if(i % 4 == 1):\n",
    "        embeded_vocab = w2v.GetEmbedding([word_to_index(word) for word in words_to_encode]).numpy()\n",
    "        \n",
    "        beacon_words = []\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('one')]).numpy(), 'one'))\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('take')]).numpy(), 'take'))\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('he')]).numpy(), 'he'))\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('have')]).numpy(), 'have'))\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('monday')]).numpy(), 'monday'))\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('iraq')]).numpy(), 'iraq'))\n",
    "        beacon_words.append((w2v.GetEmbedding([word_to_index('war')]).numpy(), 'war'))\n",
    "    \n",
    "        for embedding, word in beacon_words:\n",
    "            indexes = np.argsort(np.linalg.norm(embeded_vocab - embedding, axis=1))\n",
    "                \n",
    "            print(\"\\nClose to \\\"\" + word + '\\\" is:')\n",
    "            print(np.array(words_to_encode)[indexes][1:11])\n",
    "    \n",
    "    dataset = shuffle(dataset)\n",
    "    \n",
    "    for context, target in dataset:\n",
    "        \n",
    "        context_ix = autograd.Variable(torch.LongTensor([word_to_index(contex_word) for contex_word in context]), requires_grad=False)\n",
    "        \n",
    "        scheduler.optimizer.zero_grad()\n",
    "        \n",
    "        prediction = w2v(context_ix)\n",
    "        \n",
    "        curent_loss = loss(prediction, autograd.Variable(torch.LongTensor([word_to_index(target)]), requires_grad=False))\n",
    "        \n",
    "        curent_loss.backward()\n",
    "        scheduler.optimizer.step()\n",
    "        \n",
    "        total_loss[-1] += curent_loss.data[0]\n",
    "    \n",
    "    print(\"Epoch {:2d}. lr = {:.5f}. Loss: {:.0f}. Time for epoch: {:.1f} s. ETA: {:.0f} s\".format(i, scheduler.get_lr()[0],\n",
    "                                                                                                    np.round(total_loss[-1], -4), \n",
    "                                                                                      time.time() - epoch_start_time, \n",
    "                                                                      (EPOCH - i) * (time.time() - start_time) / (i + 1)))\n",
    "    total_loss.append(0)\n",
    "    scheduler.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(EPOCH), total_loss[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[25]['events'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_to_code = {'Life':1, 'Movement':2, 'Transaction':3, 'Business':4, 'Conflict':5, 'Contact':6, 'Personnel':7, 'Justice':8}\n",
    "\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "for document in data:\n",
    "    text = document['text']\n",
    "    \n",
    "    text = re.sub(re.compile(r\"<HEADLINE>.*</HEADLINE>\", re.DOTALL), '', text)\n",
    "    text = text.replace('<BODY>', '')\n",
    "    text = text.replace('</BODY>', '')\n",
    "    text = re.sub(re.compile(r\"<SPEAKER>(.*?)</SPEAKER>\", re.DOTALL), '', text)\n",
    "    text = text.replace('<TURN>', '')\n",
    "    text = text.replace('</TURN>', '')\n",
    "    text = text.replace('<TEXT>', '')\n",
    "    text = text.replace('</TEXT>', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\\\', '')\n",
    "    \n",
    "    X.append(text)\n",
    "    y.append(np.zeros(len(text)))\n",
    "    \n",
    "    for event in document['events']:\n",
    "        for mention in event['MENTIONS']:\n",
    "            clean_text = mention['scope_text'].replace('\\n', ' ')\n",
    "            clean_text = clean_text.replace('\\\\', '')\n",
    "            real_scope_start = text.find(clean_text)\n",
    "            anchor_shift = mention['anchor_START'] - mention['scope_START'] \n",
    "            anchor_lenght = mention['anchor_END'] - mention['anchor_START'] \n",
    "            y[-1][real_scope_start+anchor_shift:real_scope_start+anchor_shift + anchor_lenght + 1] = Type_to_code[event['TYPE']]\n",
    "    \n",
    "    X[-1] = X[-1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[480] = X[479]\n",
    "y[480] = y[479]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = []\n",
    "\n",
    "for single_y in y:\n",
    "    y_bin.append((single_y > 0).astype(np.int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_lowercase + ' ,.?!;:-_$%\\'\\\"\\/'\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def arrayToTensor(array, CHUNK_SIZE):\n",
    "    tensor = torch.zeros(CHUNK_SIZE, len(array), n_letters)\n",
    "    for i in range(len(array)):\n",
    "        for li, letter in enumerate(array[i]):\n",
    "            tensor[li][i][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_of_classes, num_of_layers, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.num_of_layers = num_of_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers = num_of_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        self.hidden2label = nn.Linear(2*hidden_dim, num_of_classes)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        h0 = autograd.Variable(torch.zeros(2*self.num_of_layers, self.batch_size, self.hidden_dim))\n",
    "        c0 = autograd.Variable(torch.zeros(2*self.num_of_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = autograd.Variable(lineToTensor(sentence), requires_grad=False)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "            \n",
    "        pre_y = self.hidden2label(lstm_out.view(-1, self.hidden_dim * 2))\n",
    "        y = nn.functional.softmax(pre_y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_data(CHUNK_SIZE):\n",
    "    Splited_X = []\n",
    "    Splited_y_bin = []\n",
    "    Splited_y = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        chunks = len(X[i]) // CHUNK_SIZE\n",
    "\n",
    "        for j in range(chunks):\n",
    "            Splited_X.append(X[i][j*CHUNK_SIZE:(j+1)*CHUNK_SIZE])\n",
    "            Splited_y_bin.append(y_bin[i][j*CHUNK_SIZE:(j+1)*CHUNK_SIZE])\n",
    "            Splited_y.append(y[i][j*CHUNK_SIZE:(j+1)*CHUNK_SIZE])\n",
    "\n",
    "#         if(len(X[i]) - chunks*CHUNK_SIZE > 50):\n",
    "#             Splited_X.append(X[i][chunks*CHUNK_SIZE:])\n",
    "#             Splited_y_bin.append(y_bin[i][chunks*CHUNK_SIZE:])\n",
    "#             Splited_y.append(y[i][chunks*CHUNK_SIZE:])\n",
    "            \n",
    "    return Splited_X, Splited_y, Splited_y_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embedding_dim=41, hidden_dim=50, num_of_classes=2, \n",
    "                           num_of_layers=1, batch_size=99)\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1, 50]))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Splited_X, Splited_y, Splited_y_bin = Split_data(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_new = autograd.Variable(arrayToTensor(Splited_X[5*99:(5+1)*99], 150), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "model.hidden = model.init_hidden()\n",
    "\n",
    "pred = model(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(CHUNK_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_OF_CLASSES, NUM_OF_LAYERS, EPOCH, BATCH_SIZE = 20):\n",
    "\n",
    "    VALIDATION_SIZE = 0.1\n",
    "    \n",
    "    Splited_X, Splited_y, Splited_y_bin = Split_data(CHUNK_SIZE)\n",
    "    Splited_X, Splited_y, Splited_y_bin = shuffle(Splited_X, Splited_y, Splited_y_bin)\n",
    "\n",
    "    Validation_X, Validation_y, Validation_y_bin = Splited_X[:int(VALIDATION_SIZE * len(Splited_X))], Splited_y[:int(VALIDATION_SIZE * len(Splited_X))], Splited_y_bin[:int(VALIDATION_SIZE * len(Splited_X))]\n",
    "    Training_X, Training_y, Training_y_bin = Splited_X[int(VALIDATION_SIZE * len(Splited_X)) :], Splited_y[int(VALIDATION_SIZE * len(Splited_X)) :], Splited_y_bin[int(VALIDATION_SIZE * len(Splited_X)) :]\n",
    "    \n",
    "    model = LSTMClassifier(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_of_classes=NUM_OF_CLASSES, \n",
    "                           num_of_layers=NUM_OF_LAYERS, batch_size=BATCH_SIZE)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1, 50]))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "    losses = [0]\n",
    "\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for epoch in range(EPOCH):  \n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "\n",
    "        Training_X, Training_y, Training_y_bin = shuffle(Training_X, Training_y, Training_y_bin)\n",
    "\n",
    "        for i in range(min(len(Training_X) // BATCH_SIZE, 1000)):\n",
    "            X = autograd.Variable(arrayToTensor(Training_X[i*BATCH_SIZE:(i+1)*BATCH_SIZE], CHUNK_SIZE), requires_grad=False)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            model.hidden = model.init_hidden()\n",
    "            \n",
    "            pred = model(X)\n",
    "            \n",
    "            loss = loss_function(pred, autograd.Variable(torch.LongTensor(Training_y_bin[i*BATCH_SIZE:(i+1)*BATCH_SIZE]).view(-1)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[-1] += loss.data[0]\n",
    "            \n",
    "        TP = 0\n",
    "        FP = 1\n",
    "        FN = 1\n",
    "        TN = 0\n",
    "        \n",
    "        for i in range(len(Validation_X) // BATCH_SIZE):\n",
    "            X = autograd.Variable(arrayToTensor(Validation_X[i*BATCH_SIZE:(i+1)*BATCH_SIZE], CHUNK_SIZE), requires_grad=False)\n",
    "            \n",
    "            model.hidden = model.init_hidden()\n",
    "            \n",
    "            pred = model(X)\n",
    "            \n",
    "            for prediction_b, true_prediction_b in zip(pred.data.tolist(), Validation_y_bin[i*BATCH_SIZE:(i+1)*BATCH_SIZE]):\n",
    "                for prediction, true_prediction in zip(prediction_b, true_prediction_b):\n",
    "                    predicted_label = np.argmax(prediction)\n",
    "                    if(predicted_label == 1 and true_prediction == 1):\n",
    "                        TP += 1\n",
    "                    elif(predicted_label == 1 and true_prediction == 0):\n",
    "                        FP += 1\n",
    "                    elif(predicted_label == 0 and true_prediction == 1):\n",
    "                        FN += 1\n",
    "                    elif(predicted_label == 0 and true_prediction == 0):\n",
    "                        TN += 1    \n",
    "            \n",
    "         \n",
    "        accuracy.append((TP+TN)/(TP+TN+FN+FP))\n",
    "        precision.append(TP/(TP+FP))\n",
    "        recall.append(TP/(TP+FN))\n",
    "\n",
    "        print(\"Loss = {:.1f}, accuracy = {:.1f}%, precision = {:.1f}%, recall = {:.1f}%\".format(losses[-1], \n",
    "                                                                                                100*accuracy[-1], \n",
    "                                                                                                100*precision[-1], \n",
    "                                                                                                100*recall[-1]))\n",
    "        losses.append(0)\n",
    "        \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Results(precision, recall):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.title(\"Binary event classification\")\n",
    "    plt.plot(precision)\n",
    "    plt.plot(recall)\n",
    "    F1 = 2. * np.array(precision) * np.array(recall) / (np.array(precision) + np.array(recall))\n",
    "    plt.plot(F1)\n",
    "    plt.legend([\"Precision (max = {:.2f})\".format(np.max(precision)), \n",
    "                \"Recall (max = {:.2f})\".format(np.max(recall)), \n",
    "                \"F1 score (max = {:.2f})\".format(np.max(F1))])\n",
    "    plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 41\n",
    "\n",
    "CHUNK_SIZE = 150\n",
    "HIDDEN_DIM = 50\n",
    "NUM_OF_CLASSES = 2\n",
    "EPOCH = 50\n",
    "NUM_OF_LAYERS = 1\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "accuracy1, precision1, recall1 = train(CHUNK_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_OF_CLASSES, NUM_OF_LAYERS, EPOCH, BATCH_SIZE)\n",
    "Plot_Results(precision1, recall1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 150\n",
    "HIDDEN_DIM = 50\n",
    "NUM_OF_CLASSES = 2\n",
    "EPOCH = 50\n",
    "NUM_OF_LAYERS = 2\n",
    "\n",
    "accuracy2, precision2, recall2 = train(CHUNK_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_OF_CLASSES, NUM_OF_LAYERS, EPOCH)\n",
    "Plot_Results(precision2, recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 150\n",
    "HIDDEN_DIM = 50\n",
    "NUM_OF_CLASSES = 2\n",
    "EPOCH = 50\n",
    "NUM_OF_LAYERS = 3\n",
    "\n",
    "accuracy3, precision3, recall3 = train(CHUNK_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_OF_CLASSES, NUM_OF_LAYERS, EPOCH)\n",
    "Plot_Results(precision3, recall3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 150\n",
    "HIDDEN_DIM = 50\n",
    "NUM_OF_CLASSES = 2\n",
    "EPOCH = 50\n",
    "NUM_OF_LAYERS = 4\n",
    "\n",
    "accuracy4, precision4, recall4 = train(CHUNK_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_OF_CLASSES, NUM_OF_LAYERS, EPOCH)\n",
    "Plot_Results(precision4, recall4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "F11 = 2. * np.array(precision1) * np.array(recall1) / (np.array(precision1) + np.array(recall1))\n",
    "F12 = 2. * np.array(precision2) * np.array(recall2) / (np.array(precision2) + np.array(recall2))\n",
    "F13 = 2. * np.array(precision3) * np.array(recall3) / (np.array(precision3) + np.array(recall3))\n",
    "F14 = 2. * np.array(precision4) * np.array(recall4) / (np.array(precision4) + np.array(recall4))\n",
    "\n",
    "plt.title(\"F1 scores for different sizes of text chunks\")\n",
    "plt.plot(F11)\n",
    "plt.plot(F12)\n",
    "plt.plot(F13)\n",
    "plt.plot(F14)\n",
    "plt.legend([\"100\", \"150\", \"200\", \"250\"])\n",
    "plt.xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 150\n",
    "HIDDEN_DIM = 150\n",
    "NUM_OF_CLASSES = 2\n",
    "EPOCH = 100\n",
    "\n",
    "accuracy5, precision5, recall5 = train(CHUNK_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_OF_CLASSES, NUM_OF_LAYERS, EPOCH)\n",
    "Plot_Results(precision1, recall1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
